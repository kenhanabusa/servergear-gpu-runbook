#!/usr/bin/env bash
timestamp(){ date +%Y%m%d_%H%M%S; }
    set -euo pipefail

SCRIPT_DIR="$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")" && pwd)"
RB_COMMON_SYSTEM="/usr/local/lib/sg-runbook/lib/common.sh"
RB_COMMON_REPO="$(cd -- "${SCRIPT_DIR}/.." && pwd)/lib/common.sh"
if [[ -r "$RB_COMMON_SYSTEM" ]]; then
  # shellcheck source=/usr/local/lib/sg-runbook/lib/common.sh
  . "$RB_COMMON_SYSTEM"
elif [[ -r "$RB_COMMON_REPO" ]]; then
  # shellcheck source=runbooks/lib/common.sh
  . "$RB_COMMON_REPO"
else
  echo "[ERROR] common.sh not found: $RB_COMMON_SYSTEM or $RB_COMMON_REPO" >&2
  exit 1
fi

    require_root

    RB_LOG_DIR="/var/log/sg-runbook"
    ensure_log_dir "$RB_LOG_DIR"
    LOG="$RB_LOG_DIR/install_genai_demo_$(timestamp).log"
    exec > >(tee -a "$LOG") 2>&1

    demo_dir="/opt/sg-demos/genai-textgen"
    sif=""
    skip_pip=0

    usage() {
      cat <<'USAGE'
    Usage:
      sg-install-genai-demo [--demo-dir DIR] [--sif PATH] [--skip-pip]

    What this does:
      - Creates a small "text generation" demo under /opt/sg-demos/genai-textgen
      - (default) Installs Python deps into a local, removable directory:
          /opt/sg-demos/genai-textgen/pydeps
      - Keeps Hugging Face cache inside the demo directory so sg-remove-genai-demo
        can clean it fully.

    Prereqs:
      - Apptainer installed
      - An NGC PyTorch SIF already pulled (see sg-install-ngc-container)
        Default expected SIF path:
          /opt/sg-images/nvcr.io_nvidia_pytorch_latest.sif

    Options:
      --demo-dir DIR   (default: /opt/sg-demos/genai-textgen)
      --sif PATH       SIF path (default: /opt/sg-images/nvcr.io_nvidia_pytorch_latest.sif)
      --skip-pip       do NOT install python deps (only write demo files)
      -h, --help       show this help

    Notes:
      - This runbook does NOT require your NGC API key (it uses a local SIF).
      - Internet access is required at first run to download the model weights
        (cached under demo_dir/hf_home).

USAGE
    }

    while [[ $# -gt 0 ]]; do
      case "$1" in
        --demo-dir) demo_dir="$2"; shift 2;;
        --sif) sif="$2"; shift 2;;
        --skip-pip) skip_pip=1; shift;;
        -h|--help) usage; exit 0;;
        *) die "unknown arg: $1 (try --help)";;
      esac
    done

    have_cmd apptainer || die "apptainer not found (install apptainer first)."

    if [[ -z "$sif" ]]; then
      sif="/opt/sg-images/nvcr.io_nvidia_pytorch_latest.sif"
    fi
    [[ -f "$sif" ]] || die "SIF not found: $sif (run sg-install-ngc-container --image nvcr.io/nvidia/pytorch --tag latest)"

    pydeps="${demo_dir}/pydeps"
    hf_home="${demo_dir}/hf_home"
    mkdir -p "$demo_dir" "$pydeps" "$hf_home"

    cat > "${demo_dir}/env.sh" <<EOF
    # Generated by sg-install-genai-demo
    export SG_DEMO_DIR="${demo_dir}"
    export SG_DEMO_PYDEPS="${pydeps}"
    export HF_HOME="${hf_home}"
    export TRANSFORMERS_CACHE="${hf_home}/transformers"
    export HF_HUB_CACHE="${hf_home}/hub"
    export XDG_CACHE_HOME="${hf_home}/xdg"
EOF
    chmod 0644 "${demo_dir}/env.sh"

    cat > "${demo_dir}/demo_textgen.py" <<'PY'
    import os
    import sys
    import time

    def main():
        print("== sg genai textgen demo ==")
        print("python:", sys.version.replace("\n"," "))

        try:
            import torch
        except Exception as e:
            print("ERROR: torch import failed:", repr(e))
            raise

        print("torch:", getattr(torch, "__version__", "unknown"))
        print("torch.cuda.is_available:", torch.cuda.is_available())

        if torch.cuda.is_available():
            try:
                dev = torch.cuda.current_device()
                name = torch.cuda.get_device_name(dev)
                print(f"cuda device: {dev} / {name}")
            except Exception as e:
                print("WARN: could not query cuda device name:", repr(e))

        # Try transformers-based generation.
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
        except Exception as e:
            print("ERROR: transformers import failed:", repr(e))
            print("Hint: run sg-install-genai-demo (without --skip-pip) to install deps into ./pydeps")
            raise

        model_id = os.environ.get("SG_DEMO_MODEL", "gpt2")
        prompt = os.environ.get("SG_DEMO_PROMPT", "Once upon a time,")
        max_new = int(os.environ.get("SG_DEMO_MAX_NEW_TOKENS", "80"))

        print("model:", model_id)
        print("prompt:", prompt)

        t0 = time.time()
        tok = AutoTokenizer.from_pretrained(model_id)
        model = AutoModelForCausalLM.from_pretrained(model_id)

        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(device)

        inputs = tok(prompt, return_tensors="pt").to(device)

        with torch.no_grad():
            out = model.generate(
                **inputs,
                do_sample=True,
                top_p=0.95,
                temperature=0.8,
                max_new_tokens=max_new,
            )

        text = tok.decode(out[0], skip_special_tokens=True)
        dt = time.time() - t0

        print("\n== generated text ==")
        print(text)
        print("\n(seconds):", round(dt, 3))

    if __name__ == "__main__":
        main()
PY
    chmod 0644 "${demo_dir}/demo_textgen.py"

    log "Wrote demo to: $demo_dir"
    log "SIF: $sif"

# SG-RUNBOOK:AUTODEDENT (auto-fix indentation in generated python files)
DEMO_DIR="$demo_dir" python3 - <<'PYDEDENT'
import os, glob, textwrap

demo_dir = os.environ.get("DEMO_DIR", "/opt/sg-demos/genai-textgen")
paths = sorted(glob.glob(os.path.join(demo_dir, "*.py")))
for path in paths:
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        t = f.read().replace("\r\n", "\n").replace("\r", "\n")
    t2 = textwrap.dedent(t)
    if t2 != t:
        with open(path, "w", encoding="utf-8") as f:
            f.write(t2)
        print(f"[INFO] dedented: {path}")
PYDEDENT


    if (( skip_pip )); then
      warn "--skip-pip set: python deps NOT installed."
      log "Done."
      exit 0
    fi

    log "Installing python deps into: $pydeps (inside container)"
    log "This uses pip inside the container and requires internet access."

    # Use apptainer exec + bind so installs land on the host filesystem.
    run apptainer exec --nv \
      --bind "$demo_dir:$demo_dir" \
      --env PIP_DISABLE_PIP_VERSION_CHECK=1 \
      --env PIP_NO_CACHE_DIR=1 \
      "$sif" \
      python3 -m pip install --upgrade --target "$pydeps" transformers sentencepiece

    log "Done."
